network {
  name = "batch_norm_act_func"
  input_rows = 8
  input_cols = 8
  input_height = 2

  layer bn {
    type = BATCH_NORM
    activation = TANH
  }

  layer bn0 {
    type = BATCH_NORM
    activation = ELU
  }

  layer bn1 {
    type = BATCH_NORM
    activation = SELU
  }

  layer fc {
    type = INNER_PRODUCT
    inner_product_param {
      num_output = 10
    }
  }
}

device {
  use_hw_pooling = true
  use_hw_batch_norm = true
  use_hw_activation_func = true
}
